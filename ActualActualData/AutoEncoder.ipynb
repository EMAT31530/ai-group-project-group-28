{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "cdc3d87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras import metrics, Input\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4e8559a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [metrics.RootMeanSquaredError(name='rms'), metrics.MeanAbsoluteError(name='mae')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "06519c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODING_DIM = 16 #Desired Dimension\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ce139978",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_and_train_autoencoder(X_train, metrics=METRICS):\n",
    "    \n",
    "    len_input_output = X_train.shape[-1]\n",
    "    input_ = Input(shape=(len_input_output,))\n",
    "    encoded = Dense(units=ENCODING_DIM*2, activation=\"relu\")(input_)\n",
    "    bottleneck = Dense(units=ENCODING_DIM, \n",
    "                       activation=\"relu\")(encoded)\n",
    "    decoded = Dense(units=ENCODING_DIM*2, \n",
    "                    activation=\"relu\")(bottleneck)\n",
    "    output = Dense(units=len_input_output, \n",
    "                    activation=\"linear\")(decoded)\n",
    "    #Training is performed on the entire autoencoder\n",
    "    autoencoder = Model(inputs=input_, outputs=output)\n",
    "    autoencoder.compile(optimizer='adam', loss='mean_squared_error',\n",
    "                        metrics=[metrics])\n",
    "    autoencoder.fit(X_train, X_train,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS)\n",
    "    #Use only the encoder part for dimensionality reduction\n",
    "    encoder = Model(inputs=input_, outputs=bottleneck)\n",
    "    return autoencoder, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "27ec1783",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('training_ROS.csv')\n",
    "qw = pd.read_csv('validation_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "038eabc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df.iloc[:,:-1], df.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3150b49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "8178/8178 [==============================] - 59s 7ms/step - loss: 0.0015 - rms: 0.0389 - mae: 0.0186\n",
      "Epoch 2/50\n",
      "8178/8178 [==============================] - 52s 6ms/step - loss: 3.4841e-04 - rms: 0.0187 - mae: 0.0090\n",
      "Epoch 3/50\n",
      "8178/8178 [==============================] - 52s 6ms/step - loss: 3.3358e-04 - rms: 0.0183 - mae: 0.0087\n",
      "Epoch 4/50\n",
      "8178/8178 [==============================] - 53s 6ms/step - loss: 3.2797e-04 - rms: 0.0181 - mae: 0.0085\n",
      "Epoch 5/50\n",
      "8178/8178 [==============================] - 52s 6ms/step - loss: 3.2459e-04 - rms: 0.0180 - mae: 0.0083\n",
      "Epoch 6/50\n",
      "8178/8178 [==============================] - 53s 6ms/step - loss: 3.2171e-04 - rms: 0.0179 - mae: 0.0082\n",
      "Epoch 7/50\n",
      "8178/8178 [==============================] - 53s 6ms/step - loss: 3.1976e-04 - rms: 0.0179 - mae: 0.0081\n",
      "Epoch 8/50\n",
      "8178/8178 [==============================] - 52s 6ms/step - loss: 3.1761e-04 - rms: 0.0178 - mae: 0.0080\n",
      "Epoch 9/50\n",
      "8178/8178 [==============================] - 53s 6ms/step - loss: 3.1685e-04 - rms: 0.0178 - mae: 0.0080\n",
      "Epoch 10/50\n",
      "8178/8178 [==============================] - 53s 6ms/step - loss: 3.1567e-04 - rms: 0.0178 - mae: 0.0080\n",
      "Epoch 11/50\n",
      "8178/8178 [==============================] - 57s 7ms/step - loss: 3.1476e-04 - rms: 0.0177 - mae: 0.0079\n",
      "Epoch 12/50\n",
      "8178/8178 [==============================] - 45s 6ms/step - loss: 3.1438e-04 - rms: 0.0177 - mae: 0.0079\n",
      "Epoch 13/50\n",
      "8178/8178 [==============================] - 53s 6ms/step - loss: 3.1385e-04 - rms: 0.0177 - mae: 0.0078\n",
      "Epoch 14/50\n",
      "8178/8178 [==============================] - 53s 6ms/step - loss: 3.1402e-04 - rms: 0.0177 - mae: 0.0079\n",
      "Epoch 15/50\n",
      "8178/8178 [==============================] - 55s 7ms/step - loss: 3.1336e-04 - rms: 0.0177 - mae: 0.0078\n",
      "Epoch 16/50\n",
      "8178/8178 [==============================] - 54s 7ms/step - loss: 3.1254e-04 - rms: 0.0177 - mae: 0.0078\n",
      "Epoch 17/50\n",
      "8178/8178 [==============================] - 56s 7ms/step - loss: 3.1181e-04 - rms: 0.0177 - mae: 0.0078\n",
      "Epoch 18/50\n",
      "8178/8178 [==============================] - 55s 7ms/step - loss: 3.1139e-04 - rms: 0.0176 - mae: 0.0078\n",
      "Epoch 19/50\n",
      "8178/8178 [==============================] - 55s 7ms/step - loss: 3.1043e-04 - rms: 0.0176 - mae: 0.0077\n",
      "Epoch 20/50\n",
      "8178/8178 [==============================] - 55s 7ms/step - loss: 3.0782e-04 - rms: 0.0175 - mae: 0.0078\n",
      "Epoch 21/50\n",
      "8178/8178 [==============================] - 54s 7ms/step - loss: 3.0656e-04 - rms: 0.0175 - mae: 0.0078\n",
      "Epoch 22/50\n",
      "8178/8178 [==============================] - 53s 7ms/step - loss: 3.0608e-04 - rms: 0.0175 - mae: 0.0078\n",
      "Epoch 23/50\n",
      "8178/8178 [==============================] - 54s 7ms/step - loss: 3.0478e-04 - rms: 0.0175 - mae: 0.0078\n",
      "Epoch 24/50\n",
      "8178/8178 [==============================] - 54s 7ms/step - loss: 3.0425e-04 - rms: 0.0174 - mae: 0.0077\n",
      "Epoch 25/50\n",
      "8178/8178 [==============================] - 55s 7ms/step - loss: 3.0015e-04 - rms: 0.0173 - mae: 0.0077\n",
      "Epoch 26/50\n",
      "8178/8178 [==============================] - 54s 7ms/step - loss: 2.9844e-04 - rms: 0.0173 - mae: 0.0077\n",
      "Epoch 27/50\n",
      "8178/8178 [==============================] - 53s 7ms/step - loss: 2.9643e-04 - rms: 0.0172 - mae: 0.0076\n",
      "Epoch 28/50\n",
      "8178/8178 [==============================] - 53s 7ms/step - loss: 2.9591e-04 - rms: 0.0172 - mae: 0.0076\n",
      "Epoch 29/50\n",
      "8178/8178 [==============================] - 55s 7ms/step - loss: 2.9567e-04 - rms: 0.0172 - mae: 0.0076\n",
      "Epoch 30/50\n",
      "8178/8178 [==============================] - 59s 7ms/step - loss: 2.9600e-04 - rms: 0.0172 - mae: 0.0076\n",
      "Epoch 31/50\n",
      "8178/8178 [==============================] - 52s 6ms/step - loss: 2.9493e-04 - rms: 0.0172 - mae: 0.0076\n",
      "Epoch 32/50\n",
      "8178/8178 [==============================] - 53s 6ms/step - loss: 2.9497e-04 - rms: 0.0172 - mae: 0.0076\n",
      "Epoch 33/50\n",
      "8178/8178 [==============================] - 52s 6ms/step - loss: 2.9542e-04 - rms: 0.0172 - mae: 0.0076\n",
      "Epoch 34/50\n",
      "8178/8178 [==============================] - 54s 7ms/step - loss: 2.9418e-04 - rms: 0.0172 - mae: 0.0075\n",
      "Epoch 35/50\n",
      "8178/8178 [==============================] - 57s 7ms/step - loss: 2.9529e-04 - rms: 0.0172 - mae: 0.0076\n",
      "Epoch 36/50\n",
      "8178/8178 [==============================] - 54s 7ms/step - loss: 2.9532e-04 - rms: 0.0172 - mae: 0.0076\n",
      "Epoch 37/50\n",
      "8178/8178 [==============================] - 55s 7ms/step - loss: 2.9505e-04 - rms: 0.0172 - mae: 0.0076\n",
      "Epoch 38/50\n",
      "8178/8178 [==============================] - 53s 7ms/step - loss: 2.9539e-04 - rms: 0.0172 - mae: 0.0076\n",
      "Epoch 39/50\n",
      "8178/8178 [==============================] - 54s 7ms/step - loss: 2.9521e-04 - rms: 0.0172 - mae: 0.0076\n",
      "Epoch 40/50\n",
      "8178/8178 [==============================] - 55s 7ms/step - loss: 2.9340e-04 - rms: 0.0171 - mae: 0.0075\n",
      "Epoch 41/50\n",
      "8178/8178 [==============================] - 54s 7ms/step - loss: 2.9412e-04 - rms: 0.0172 - mae: 0.0075\n",
      "Epoch 42/50\n",
      "8178/8178 [==============================] - 53s 7ms/step - loss: 2.9395e-04 - rms: 0.0171 - mae: 0.0075\n",
      "Epoch 43/50\n",
      "8178/8178 [==============================] - 53s 7ms/step - loss: 2.9381e-04 - rms: 0.0171 - mae: 0.0075\n",
      "Epoch 44/50\n",
      "8178/8178 [==============================] - 54s 7ms/step - loss: 2.9411e-04 - rms: 0.0171 - mae: 0.0075\n",
      "Epoch 45/50\n",
      "8178/8178 [==============================] - 54s 7ms/step - loss: 2.9449e-04 - rms: 0.0172 - mae: 0.0075\n",
      "Epoch 46/50\n",
      "8178/8178 [==============================] - 61s 7ms/step - loss: 2.9398e-04 - rms: 0.0171 - mae: 0.0075\n",
      "Epoch 47/50\n",
      "8178/8178 [==============================] - 62s 8ms/step - loss: 2.9375e-04 - rms: 0.0171 - mae: 0.0075\n",
      "Epoch 48/50\n",
      "8178/8178 [==============================] - 63s 8ms/step - loss: 2.9383e-04 - rms: 0.0171 - mae: 0.0075\n",
      "Epoch 49/50\n",
      "8178/8178 [==============================] - 58s 7ms/step - loss: 2.9380e-04 - rms: 0.0171 - mae: 0.0075\n",
      "Epoch 50/50\n",
      "8178/8178 [==============================] - 55s 7ms/step - loss: 2.9276e-04 - rms: 0.0171 - mae: 0.0075\n"
     ]
    }
   ],
   "source": [
    "auto, encode = make_and_train_autoencoder(X, metrics=METRICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5e0d83bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = encode.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ef57413b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32710, 16)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2b308b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = np.array(q)\n",
    "df = pd.DataFrame(data=q,index=None,columns=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "43b0fc90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.176377</td>\n",
       "      <td>0.297052</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.168536</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.091217</td>\n",
       "      <td>0.177422</td>\n",
       "      <td>0.192657</td>\n",
       "      <td>0.114414</td>\n",
       "      <td>0.042181</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.303161</td>\n",
       "      <td>0.158141</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.164332</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.145195</td>\n",
       "      <td>0.334893</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.130104</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.108132</td>\n",
       "      <td>0.276668</td>\n",
       "      <td>0.220707</td>\n",
       "      <td>0.228691</td>\n",
       "      <td>0.136393</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.342791</td>\n",
       "      <td>0.180877</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.196175</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.315034</td>\n",
       "      <td>0.246673</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.349820</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.284155</td>\n",
       "      <td>0.224775</td>\n",
       "      <td>0.130678</td>\n",
       "      <td>0.121905</td>\n",
       "      <td>0.173306</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.310383</td>\n",
       "      <td>0.088846</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.401038</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.155673</td>\n",
       "      <td>0.372546</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.313895</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250187</td>\n",
       "      <td>0.228466</td>\n",
       "      <td>0.118142</td>\n",
       "      <td>0.129902</td>\n",
       "      <td>0.078225</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.400816</td>\n",
       "      <td>0.140651</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.108406</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.178996</td>\n",
       "      <td>0.244286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.225383</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.261496</td>\n",
       "      <td>0.113891</td>\n",
       "      <td>0.078897</td>\n",
       "      <td>0.074989</td>\n",
       "      <td>0.069801</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.212734</td>\n",
       "      <td>0.120407</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.058746</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32705</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.200497</td>\n",
       "      <td>0.121754</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.119313</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.280635</td>\n",
       "      <td>0.120347</td>\n",
       "      <td>0.068865</td>\n",
       "      <td>0.063566</td>\n",
       "      <td>0.045507</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.094394</td>\n",
       "      <td>0.161756</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.071938</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32706</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.354684</td>\n",
       "      <td>0.316169</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.300437</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.122386</td>\n",
       "      <td>0.171823</td>\n",
       "      <td>0.259745</td>\n",
       "      <td>0.170999</td>\n",
       "      <td>0.209344</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.518709</td>\n",
       "      <td>0.172038</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.485416</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32707</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.331742</td>\n",
       "      <td>0.354997</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.313935</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.091947</td>\n",
       "      <td>0.291601</td>\n",
       "      <td>0.205047</td>\n",
       "      <td>0.139764</td>\n",
       "      <td>0.199323</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.289754</td>\n",
       "      <td>0.222683</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.496981</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32708</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.324100</td>\n",
       "      <td>0.248483</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.334480</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.270242</td>\n",
       "      <td>0.115537</td>\n",
       "      <td>0.002554</td>\n",
       "      <td>0.023827</td>\n",
       "      <td>0.143548</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.096559</td>\n",
       "      <td>0.086054</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.354157</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32709</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.281842</td>\n",
       "      <td>0.359974</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.265191</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.272565</td>\n",
       "      <td>0.228868</td>\n",
       "      <td>0.272089</td>\n",
       "      <td>0.175516</td>\n",
       "      <td>0.035595</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.597491</td>\n",
       "      <td>0.286307</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.183078</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32710 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2    3         4    5         6         7  \\\n",
       "0      0.0  0.176377  0.297052  0.0  0.168536  0.0  0.091217  0.177422   \n",
       "1      0.0  0.145195  0.334893  0.0  0.130104  0.0  0.108132  0.276668   \n",
       "2      0.0  0.315034  0.246673  0.0  0.349820  0.0  0.284155  0.224775   \n",
       "3      0.0  0.155673  0.372546  0.0  0.313895  0.0  0.250187  0.228466   \n",
       "4      0.0  0.178996  0.244286  0.0  0.225383  0.0  0.261496  0.113891   \n",
       "...    ...       ...       ...  ...       ...  ...       ...       ...   \n",
       "32705  0.0  0.200497  0.121754  0.0  0.119313  0.0  0.280635  0.120347   \n",
       "32706  0.0  0.354684  0.316169  0.0  0.300437  0.0  0.122386  0.171823   \n",
       "32707  0.0  0.331742  0.354997  0.0  0.313935  0.0  0.091947  0.291601   \n",
       "32708  0.0  0.324100  0.248483  0.0  0.334480  0.0  0.270242  0.115537   \n",
       "32709  0.0  0.281842  0.359974  0.0  0.265191  0.0  0.272565  0.228868   \n",
       "\n",
       "              8         9        10   11        12        13   14        15  \\\n",
       "0      0.192657  0.114414  0.042181  0.0  0.303161  0.158141  0.0  0.164332   \n",
       "1      0.220707  0.228691  0.136393  0.0  0.342791  0.180877  0.0  0.196175   \n",
       "2      0.130678  0.121905  0.173306  0.0  0.310383  0.088846  0.0  0.401038   \n",
       "3      0.118142  0.129902  0.078225  0.0  0.400816  0.140651  0.0  0.108406   \n",
       "4      0.078897  0.074989  0.069801  0.0  0.212734  0.120407  0.0  0.058746   \n",
       "...         ...       ...       ...  ...       ...       ...  ...       ...   \n",
       "32705  0.068865  0.063566  0.045507  0.0  0.094394  0.161756  0.0  0.071938   \n",
       "32706  0.259745  0.170999  0.209344  0.0  0.518709  0.172038  0.0  0.485416   \n",
       "32707  0.205047  0.139764  0.199323  0.0  0.289754  0.222683  0.0  0.496981   \n",
       "32708  0.002554  0.023827  0.143548  0.0  0.096559  0.086054  0.0  0.354157   \n",
       "32709  0.272089  0.175516  0.035595  0.0  0.597491  0.286307  0.0  0.183078   \n",
       "\n",
       "       label  \n",
       "0        0.0  \n",
       "1        1.0  \n",
       "2        0.0  \n",
       "3        0.0  \n",
       "4        0.0  \n",
       "...      ...  \n",
       "32705    1.0  \n",
       "32706    1.0  \n",
       "32707    1.0  \n",
       "32708    1.0  \n",
       "32709    1.0  \n",
       "\n",
       "[32710 rows x 17 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'] = y\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ca00f7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('training_ros_autoencoded_4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ae02cfae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2496/2496 [==============================] - 19s 7ms/step - loss: 0.0035 - rms: 0.0324 - mae: 0.0131\n",
      "Epoch 2/50\n",
      "2496/2496 [==============================] - 14s 6ms/step - loss: 4.0909e-04 - rms: 0.0202 - mae: 0.0113\n",
      "Epoch 3/50\n",
      "2496/2496 [==============================] - 14s 6ms/step - loss: 2.7897e-04 - rms: 0.0167 - mae: 0.0089\n",
      "Epoch 4/50\n",
      "2496/2496 [==============================] - 17s 7ms/step - loss: 2.0577e-04 - rms: 0.0143 - mae: 0.0070\n",
      "Epoch 5/50\n",
      "2496/2496 [==============================] - 15s 6ms/step - loss: 1.8467e-04 - rms: 0.0136 - mae: 0.0062\n",
      "Epoch 6/50\n",
      "2496/2496 [==============================] - 16s 6ms/step - loss: 1.6547e-04 - rms: 0.0129 - mae: 0.0060\n",
      "Epoch 7/50\n",
      "2496/2496 [==============================] - 17s 7ms/step - loss: 1.3380e-04 - rms: 0.0116 - mae: 0.0054\n",
      "Epoch 8/50\n",
      "2496/2496 [==============================] - 18s 7ms/step - loss: 1.2926e-04 - rms: 0.0114 - mae: 0.0054\n",
      "Epoch 9/50\n",
      "2496/2496 [==============================] - 16s 6ms/step - loss: 1.2687e-04 - rms: 0.0113 - mae: 0.0052\n",
      "Epoch 10/50\n",
      "2496/2496 [==============================] - 21s 8ms/step - loss: 1.2519e-04 - rms: 0.0112 - mae: 0.0052\n",
      "Epoch 11/50\n",
      "2496/2496 [==============================] - 22s 9ms/step - loss: 1.2688e-04 - rms: 0.0113 - mae: 0.0052\n",
      "Epoch 12/50\n",
      "2496/2496 [==============================] - 15s 6ms/step - loss: 1.2342e-04 - rms: 0.0111 - mae: 0.0050\n",
      "Epoch 13/50\n",
      "2496/2496 [==============================] - 20s 8ms/step - loss: 1.2303e-04 - rms: 0.0111 - mae: 0.0051\n",
      "Epoch 14/50\n",
      "2496/2496 [==============================] - 14s 6ms/step - loss: 1.2389e-04 - rms: 0.0111 - mae: 0.0051\n",
      "Epoch 15/50\n",
      "2496/2496 [==============================] - 14s 5ms/step - loss: 1.2280e-04 - rms: 0.0111 - mae: 0.0050\n",
      "Epoch 16/50\n",
      "2496/2496 [==============================] - 15s 6ms/step - loss: 1.2211e-04 - rms: 0.0111 - mae: 0.0050\n",
      "Epoch 17/50\n",
      "2496/2496 [==============================] - 14s 6ms/step - loss: 1.2175e-04 - rms: 0.0110 - mae: 0.0050\n",
      "Epoch 18/50\n",
      "2496/2496 [==============================] - 17s 7ms/step - loss: 1.2114e-04 - rms: 0.0110 - mae: 0.0049\n",
      "Epoch 19/50\n",
      "2496/2496 [==============================] - 14s 6ms/step - loss: 1.1982e-04 - rms: 0.0109 - mae: 0.0048\n",
      "Epoch 20/50\n",
      "2496/2496 [==============================] - 14s 5ms/step - loss: 1.2148e-04 - rms: 0.0110 - mae: 0.0049\n",
      "Epoch 21/50\n",
      "2496/2496 [==============================] - 14s 6ms/step - loss: 1.1742e-04 - rms: 0.0108 - mae: 0.0047\n",
      "Epoch 22/50\n",
      "2496/2496 [==============================] - 14s 5ms/step - loss: 1.1957e-04 - rms: 0.0109 - mae: 0.0048\n",
      "Epoch 23/50\n",
      "2496/2496 [==============================] - 16s 6ms/step - loss: 1.2113e-04 - rms: 0.0110 - mae: 0.0049\n",
      "Epoch 24/50\n",
      "2496/2496 [==============================] - 16s 6ms/step - loss: 1.1795e-04 - rms: 0.0109 - mae: 0.0047\n",
      "Epoch 25/50\n",
      "2496/2496 [==============================] - 16s 6ms/step - loss: 1.1827e-04 - rms: 0.0109 - mae: 0.0048\n",
      "Epoch 26/50\n",
      "2496/2496 [==============================] - 16s 6ms/step - loss: 1.1844e-04 - rms: 0.0109 - mae: 0.0048\n",
      "Epoch 27/50\n",
      "2496/2496 [==============================] - 16s 6ms/step - loss: 1.1780e-04 - rms: 0.0109 - mae: 0.0047\n",
      "Epoch 28/50\n",
      "2496/2496 [==============================] - 19s 8ms/step - loss: 1.1786e-04 - rms: 0.0109 - mae: 0.0047\n",
      "Epoch 29/50\n",
      "2496/2496 [==============================] - 16s 7ms/step - loss: 1.1699e-04 - rms: 0.0108 - mae: 0.0046\n",
      "Epoch 30/50\n",
      "2496/2496 [==============================] - 16s 6ms/step - loss: 1.1677e-04 - rms: 0.0108 - mae: 0.0047\n",
      "Epoch 31/50\n",
      "2496/2496 [==============================] - 16s 6ms/step - loss: 1.1818e-04 - rms: 0.0109 - mae: 0.0048\n",
      "Epoch 32/50\n",
      "2496/2496 [==============================] - 16s 6ms/step - loss: 1.1734e-04 - rms: 0.0108 - mae: 0.0047\n",
      "Epoch 33/50\n",
      "2496/2496 [==============================] - 17s 7ms/step - loss: 1.1856e-04 - rms: 0.0109 - mae: 0.0047\n",
      "Epoch 34/50\n",
      "2496/2496 [==============================] - 16s 6ms/step - loss: 1.1665e-04 - rms: 0.0108 - mae: 0.0046\n",
      "Epoch 35/50\n",
      "2496/2496 [==============================] - 16s 6ms/step - loss: 1.1651e-04 - rms: 0.0108 - mae: 0.0046\n",
      "Epoch 36/50\n",
      "2496/2496 [==============================] - 16s 7ms/step - loss: 1.1769e-04 - rms: 0.0108 - mae: 0.0047\n",
      "Epoch 37/50\n",
      "2496/2496 [==============================] - 16s 6ms/step - loss: 1.1864e-04 - rms: 0.0109 - mae: 0.0047\n",
      "Epoch 38/50\n",
      "2496/2496 [==============================] - 16s 6ms/step - loss: 1.1490e-04 - rms: 0.0107 - mae: 0.0045\n",
      "Epoch 39/50\n",
      "2496/2496 [==============================] - 16s 7ms/step - loss: 1.1640e-04 - rms: 0.0108 - mae: 0.0046\n",
      "Epoch 40/50\n",
      "2496/2496 [==============================] - 16s 7ms/step - loss: 1.1586e-04 - rms: 0.0108 - mae: 0.0046\n",
      "Epoch 41/50\n",
      "2496/2496 [==============================] - 16s 6ms/step - loss: 1.1564e-04 - rms: 0.0108 - mae: 0.0046\n",
      "Epoch 42/50\n",
      "2496/2496 [==============================] - 16s 6ms/step - loss: 1.1747e-04 - rms: 0.0108 - mae: 0.0047\n",
      "Epoch 43/50\n",
      "2496/2496 [==============================] - 16s 7ms/step - loss: 1.1625e-04 - rms: 0.0108 - mae: 0.0046\n",
      "Epoch 44/50\n",
      "2496/2496 [==============================] - 16s 6ms/step - loss: 1.1573e-04 - rms: 0.0108 - mae: 0.0046\n",
      "Epoch 45/50\n",
      "2496/2496 [==============================] - 16s 6ms/step - loss: 1.1506e-04 - rms: 0.0107 - mae: 0.0046\n",
      "Epoch 46/50\n",
      "2496/2496 [==============================] - 17s 7ms/step - loss: 1.1592e-04 - rms: 0.0108 - mae: 0.0046\n",
      "Epoch 47/50\n",
      "2496/2496 [==============================] - 16s 7ms/step - loss: 1.1589e-04 - rms: 0.0108 - mae: 0.0046\n",
      "Epoch 48/50\n",
      "2496/2496 [==============================] - 16s 6ms/step - loss: 1.1624e-04 - rms: 0.0108 - mae: 0.0046\n",
      "Epoch 49/50\n",
      "2496/2496 [==============================] - 16s 6ms/step - loss: 1.1714e-04 - rms: 0.0108 - mae: 0.0046\n",
      "Epoch 50/50\n",
      "2496/2496 [==============================] - 17s 7ms/step - loss: 1.1356e-04 - rms: 0.0107 - mae: 0.0044\n"
     ]
    }
   ],
   "source": [
    "#### CNN autoencode\n",
    "df = pd.read_csv('training_CNN.csv')\n",
    "X, y = df.iloc[:,:-1], df.iloc[:, -1]\n",
    "auto, encode = make_and_train_autoencoder(X, metrics=METRICS)\n",
    "q = encode.predict(X)\n",
    "w = np.array(q)\n",
    "df = pd.DataFrame(data=w,index=None,columns=None)\n",
    "df['label'] = y\n",
    "df.to_csv('training_cnn_autoencoded_4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "559ffb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2323/2323 [==============================] - 22s 8ms/step - loss: 0.0033 - rms: 0.0408 - mae: 0.0172\n",
      "Epoch 2/50\n",
      "2323/2323 [==============================] - 14s 6ms/step - loss: 4.3035e-04 - rms: 0.0207 - mae: 0.0116\n",
      "Epoch 3/50\n",
      "2323/2323 [==============================] - 13s 6ms/step - loss: 2.5042e-04 - rms: 0.0158 - mae: 0.0080\n",
      "Epoch 4/50\n",
      "2323/2323 [==============================] - 13s 6ms/step - loss: 2.0362e-04 - rms: 0.0143 - mae: 0.0070\n",
      "Epoch 5/50\n",
      "2323/2323 [==============================] - 13s 6ms/step - loss: 1.7297e-04 - rms: 0.0132 - mae: 0.0064\n",
      "Epoch 6/50\n",
      "2323/2323 [==============================] - 13s 6ms/step - loss: 1.5556e-04 - rms: 0.0125 - mae: 0.0060\n",
      "Epoch 7/50\n",
      "2323/2323 [==============================] - 16s 7ms/step - loss: 1.4663e-04 - rms: 0.0121 - mae: 0.0060\n",
      "Epoch 8/50\n",
      "2323/2323 [==============================] - 18s 8ms/step - loss: 1.2748e-04 - rms: 0.0113 - mae: 0.0057\n",
      "Epoch 9/50\n",
      "2323/2323 [==============================] - 14s 6ms/step - loss: 1.1500e-04 - rms: 0.0107 - mae: 0.0055\n",
      "Epoch 10/50\n",
      "2323/2323 [==============================] - 13s 6ms/step - loss: 1.0758e-04 - rms: 0.0104 - mae: 0.0052\n",
      "Epoch 11/50\n",
      "2323/2323 [==============================] - 13s 6ms/step - loss: 1.0192e-04 - rms: 0.0101 - mae: 0.0050\n",
      "Epoch 12/50\n",
      "2323/2323 [==============================] - 13s 6ms/step - loss: 1.0071e-04 - rms: 0.0100 - mae: 0.0051\n",
      "Epoch 13/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 9.8424e-05 - rms: 0.0099 - mae: 0.0049\n",
      "Epoch 14/50\n",
      "2323/2323 [==============================] - 4196s 2s/step - loss: 9.5982e-05 - rms: 0.0098 - mae: 0.0048\n",
      "Epoch 15/50\n",
      "2323/2323 [==============================] - 13s 5ms/step - loss: 9.6137e-05 - rms: 0.0098 - mae: 0.0048\n",
      "Epoch 16/50\n",
      "2323/2323 [==============================] - 11s 5ms/step - loss: 9.5318e-05 - rms: 0.0098 - mae: 0.0048\n",
      "Epoch 17/50\n",
      "2323/2323 [==============================] - 12s 5ms/step - loss: 9.1380e-05 - rms: 0.0096 - mae: 0.0046\n",
      "Epoch 18/50\n",
      "2323/2323 [==============================] - 17s 7ms/step - loss: 9.2346e-05 - rms: 0.0096 - mae: 0.0046\n",
      "Epoch 19/50\n",
      "2323/2323 [==============================] - 14s 6ms/step - loss: 9.2685e-05 - rms: 0.0096 - mae: 0.0047\n",
      "Epoch 20/50\n",
      "2323/2323 [==============================] - 12s 5ms/step - loss: 9.2162e-05 - rms: 0.0096 - mae: 0.0047\n",
      "Epoch 21/50\n",
      "2323/2323 [==============================] - 11s 5ms/step - loss: 8.9898e-05 - rms: 0.0095 - mae: 0.0045\n",
      "Epoch 22/50\n",
      "2323/2323 [==============================] - 11s 5ms/step - loss: 9.1201e-05 - rms: 0.0095 - mae: 0.0046\n",
      "Epoch 23/50\n",
      "2323/2323 [==============================] - 11s 5ms/step - loss: 9.0342e-05 - rms: 0.0095 - mae: 0.0045\n",
      "Epoch 24/50\n",
      "2323/2323 [==============================] - 13s 5ms/step - loss: 9.0019e-05 - rms: 0.0095 - mae: 0.0046\n",
      "Epoch 25/50\n",
      "2323/2323 [==============================] - 11s 5ms/step - loss: 8.9266e-05 - rms: 0.0094 - mae: 0.0045\n",
      "Epoch 26/50\n",
      "2323/2323 [==============================] - 11s 5ms/step - loss: 9.0354e-05 - rms: 0.0095 - mae: 0.0045\n",
      "Epoch 27/50\n",
      "2323/2323 [==============================] - 13s 6ms/step - loss: 8.8585e-05 - rms: 0.0094 - mae: 0.0045\n",
      "Epoch 28/50\n",
      "2323/2323 [==============================] - 11s 5ms/step - loss: 8.8033e-05 - rms: 0.0094 - mae: 0.0044\n",
      "Epoch 29/50\n",
      "2323/2323 [==============================] - 12s 5ms/step - loss: 8.6428e-05 - rms: 0.0093 - mae: 0.0044\n",
      "Epoch 30/50\n",
      "2323/2323 [==============================] - 11s 5ms/step - loss: 8.5915e-05 - rms: 0.0093 - mae: 0.0045\n",
      "Epoch 31/50\n",
      "2323/2323 [==============================] - 11s 5ms/step - loss: 8.7442e-05 - rms: 0.0094 - mae: 0.0045\n",
      "Epoch 32/50\n",
      "2323/2323 [==============================] - 11s 5ms/step - loss: 8.9473e-05 - rms: 0.0095 - mae: 0.0046\n",
      "Epoch 33/50\n",
      "2323/2323 [==============================] - 11s 5ms/step - loss: 8.3688e-05 - rms: 0.0091 - mae: 0.0044\n",
      "Epoch 34/50\n",
      "2323/2323 [==============================] - 11s 5ms/step - loss: 8.2829e-05 - rms: 0.0091 - mae: 0.0045\n",
      "Epoch 35/50\n",
      "2323/2323 [==============================] - 11s 5ms/step - loss: 7.9565e-05 - rms: 0.0089 - mae: 0.0043\n",
      "Epoch 36/50\n",
      "2323/2323 [==============================] - 11s 5ms/step - loss: 8.6170e-05 - rms: 0.0093 - mae: 0.0045\n",
      "Epoch 37/50\n",
      "2323/2323 [==============================] - 11s 5ms/step - loss: 7.9697e-05 - rms: 0.0089 - mae: 0.0043\n",
      "Epoch 38/50\n",
      "2323/2323 [==============================] - 11s 5ms/step - loss: 8.1930e-05 - rms: 0.0091 - mae: 0.0044\n",
      "Epoch 39/50\n",
      "2323/2323 [==============================] - 11s 5ms/step - loss: 8.3710e-05 - rms: 0.0091 - mae: 0.0044\n",
      "Epoch 40/50\n",
      "2323/2323 [==============================] - 12s 5ms/step - loss: 7.6937e-05 - rms: 0.0088 - mae: 0.0042\n",
      "Epoch 41/50\n",
      "2323/2323 [==============================] - 14s 6ms/step - loss: 8.1653e-05 - rms: 0.0090 - mae: 0.0044\n",
      "Epoch 42/50\n",
      "2323/2323 [==============================] - 15s 6ms/step - loss: 8.6358e-05 - rms: 0.0093 - mae: 0.0043\n",
      "Epoch 43/50\n",
      "2323/2323 [==============================] - 16s 7ms/step - loss: 8.8129e-05 - rms: 0.0094 - mae: 0.0044\n",
      "Epoch 44/50\n",
      "2323/2323 [==============================] - 12s 5ms/step - loss: 8.0951e-05 - rms: 0.0090 - mae: 0.0044\n",
      "Epoch 45/50\n",
      "2323/2323 [==============================] - 12s 5ms/step - loss: 8.0526e-05 - rms: 0.0090 - mae: 0.0043\n",
      "Epoch 46/50\n",
      "2323/2323 [==============================] - 11s 5ms/step - loss: 7.8645e-05 - rms: 0.0089 - mae: 0.0042\n",
      "Epoch 47/50\n",
      "2323/2323 [==============================] - 11s 5ms/step - loss: 8.1044e-05 - rms: 0.0090 - mae: 0.0044\n",
      "Epoch 48/50\n",
      "2323/2323 [==============================] - 11s 5ms/step - loss: 8.2546e-05 - rms: 0.0091 - mae: 0.0044\n",
      "Epoch 49/50\n",
      "2323/2323 [==============================] - 11s 5ms/step - loss: 7.8845e-05 - rms: 0.0089 - mae: 0.0043\n",
      "Epoch 50/50\n",
      "2323/2323 [==============================] - 12s 5ms/step - loss: 8.7529e-05 - rms: 0.0094 - mae: 0.0043\n"
     ]
    }
   ],
   "source": [
    "#### CNN autoencode\n",
    "df = pd.read_csv('training_RUS.csv')\n",
    "X, y = df.iloc[:,:-1], df.iloc[:, -1]\n",
    "auto, encode = make_and_train_autoencoder(X, metrics=METRICS)\n",
    "q = encode.predict(X)\n",
    "w = np.array(q)\n",
    "df = pd.DataFrame(data=w,index=None,columns=None)\n",
    "df['label'] = y\n",
    "df.to_csv('training_rus_autoencoded_4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "620b6e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " 133/8178 [..............................] - ETA: 42s - loss: 0.0223 - rms: 0.0360 - mae: 0.0094"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [109]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining_smote.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m X, y \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39miloc[:,:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], df\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m----> 4\u001b[0m auto, encode \u001b[38;5;241m=\u001b[39m \u001b[43mmake_and_train_autoencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMETRICS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m q \u001b[38;5;241m=\u001b[39m encode\u001b[38;5;241m.\u001b[39mpredict(X)\n\u001b[0;32m      6\u001b[0m w \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(q)\n",
      "Input \u001b[1;32mIn [98]\u001b[0m, in \u001b[0;36mmake_and_train_autoencoder\u001b[1;34m(X_train, metrics)\u001b[0m\n\u001b[0;32m     13\u001b[0m autoencoder \u001b[38;5;241m=\u001b[39m Model(inputs\u001b[38;5;241m=\u001b[39minput_, outputs\u001b[38;5;241m=\u001b[39moutput)\n\u001b[0;32m     14\u001b[0m autoencoder\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     15\u001b[0m                     metrics\u001b[38;5;241m=\u001b[39m[metrics])\n\u001b[1;32m---> 16\u001b[0m \u001b[43mautoencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m                \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m                \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m#Use only the encoder part for dimensionality reduction\u001b[39;00m\n\u001b[0;32m     20\u001b[0m encoder \u001b[38;5;241m=\u001b[39m Model(inputs\u001b[38;5;241m=\u001b[39minput_, outputs\u001b[38;5;241m=\u001b[39mbottleneck)\n",
      "File \u001b[1;32mc:\\users\\thoma\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\users\\thoma\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\engine\\training.py:1376\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1374\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_epoch_begin(epoch)\n\u001b[0;32m   1375\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n\u001b[1;32m-> 1376\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39msteps():\n\u001b[0;32m   1377\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1378\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1379\u001b[0m         epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   1380\u001b[0m         step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[0;32m   1381\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1382\u001b[0m         _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1383\u001b[0m       callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n",
      "File \u001b[1;32mc:\\users\\thoma\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\engine\\data_adapter.py:1246\u001b[0m, in \u001b[0;36mDataHandler.steps\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insufficient_data:  \u001b[38;5;66;03m# Set by `catch_stop_iteration`.\u001b[39;00m\n\u001b[0;32m   1245\u001b[0m   \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1246\u001b[0m original_spe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_steps_per_execution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m   1247\u001b[0m can_run_full_execution \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1248\u001b[0m     original_spe \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1249\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferred_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1250\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferred_steps \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_step \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m\n\u001b[0;32m   1251\u001b[0m     original_spe)\n\u001b[0;32m   1253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_run_full_execution:\n",
      "File \u001b[1;32mc:\\users\\thoma\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:674\u001b[0m, in \u001b[0;36mBaseResourceVariable.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnumpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 674\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    675\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    676\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy() is only available when eager execution is enabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\users\\thoma\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:749\u001b[0m, in \u001b[0;36mBaseResourceVariable.read_value\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    740\u001b[0m \u001b[38;5;124;03m\"\"\"Constructs an op which reads the value of this variable.\u001b[39;00m\n\u001b[0;32m    741\u001b[0m \n\u001b[0;32m    742\u001b[0m \u001b[38;5;124;03mShould be used when there are multiple reads, or when it is desirable to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    746\u001b[0m \u001b[38;5;124;03m the read operation.\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mname_scope(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRead\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 749\u001b[0m   value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_variable_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    750\u001b[0m \u001b[38;5;66;03m# Return an identity so it can get placed on whatever device the context\u001b[39;00m\n\u001b[0;32m    751\u001b[0m \u001b[38;5;66;03m# specifies instead of the device where the variable is.\u001b[39;00m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m array_ops\u001b[38;5;241m.\u001b[39midentity(value)\n",
      "File \u001b[1;32mc:\\users\\thoma\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:728\u001b[0m, in \u001b[0;36mBaseResourceVariable._read_variable_op\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    726\u001b[0m       result \u001b[38;5;241m=\u001b[39m read_and_set_handle()\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 728\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mread_and_set_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m    731\u001b[0m   \u001b[38;5;66;03m# Note that if a control flow context is active the input of the read op\u001b[39;00m\n\u001b[0;32m    732\u001b[0m   \u001b[38;5;66;03m# might not actually be the handle. This line bypasses it.\u001b[39;00m\n\u001b[0;32m    733\u001b[0m   tape\u001b[38;5;241m.\u001b[39mrecord_operation(\n\u001b[0;32m    734\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReadVariableOp\u001b[39m\u001b[38;5;124m\"\u001b[39m, [result], [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle],\n\u001b[0;32m    735\u001b[0m       backward_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: [x],\n\u001b[0;32m    736\u001b[0m       forward_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: [x])\n",
      "File \u001b[1;32mc:\\users\\thoma\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:718\u001b[0m, in \u001b[0;36mBaseResourceVariable._read_variable_op.<locals>.read_and_set_handle\u001b[1;34m()\u001b[0m\n\u001b[0;32m    717\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_and_set_handle\u001b[39m():\n\u001b[1;32m--> 718\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mgen_resource_variable_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_variable_op\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    719\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    720\u001b[0m   _maybe_set_handle_data(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dtype, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle, result)\n\u001b[0;32m    721\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\users\\thoma\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py:478\u001b[0m, in \u001b[0;36mread_variable_op\u001b[1;34m(resource, dtype, name)\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m    477\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 478\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mReadVariableOp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    480\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m    481\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#### CNN autoencode\n",
    "df = pd.read_csv('validation.csv')\n",
    "X, y = df.iloc[:,:-1], df.iloc[:, -1]\n",
    "auto, encode = make_and_train_autoencoder(X, metrics=METRICS)\n",
    "q = encode.predict(X)\n",
    "w = np.array(q)\n",
    "df = pd.DataFrame(data=w,index=None,columns=None)\n",
    "df['label'] = y\n",
    "df.to_csv('training_smote_autoencoded_4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d49f10c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
